{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f20de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "# --------------------------------- IMPORTS ---------------------------------- #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# Standard Library Imports\n",
    "import io\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import time as time\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Dependency Imports\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import xgboost as xgb\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import roc_auc_score, f1_score, make_scorer, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe338bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "# ------------------------------ CONFIGURATIONS ------------------------------ #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "ROOT = \"C:/Users/TheLunes/Documents/masters_thesis/TaCLE/\"\n",
    "\n",
    "config = {\n",
    "    \"package-info\" : {\n",
    "        \"Project\" : \"Tool-assisted Classification using Lexical Elements\",\n",
    "        \"Author\" : \"Braeden Lewis\",\n",
    "        \"Language\" : \"Python v3.10.1\",\n",
    "\n",
    "        \"dependencies\" : [\n",
    "            \"nltk\",\n",
    "            \"numpy\",\n",
    "            \"pandas\",\n",
    "            \"scipy\",\n",
    "            \"scikit-learn\",\n",
    "            \"xgboost\"\n",
    "        ]\n",
    "    },\n",
    "    \"directories\" : {\n",
    "        \"ROOT\" : ROOT,\n",
    "        \"V0_DATA_IMPORT_DIR\" : Path(ROOT + \"/data/input/\"),\n",
    "        \"EXTR_PICKLE_OUTPUT_DIR\" : Path(ROOT + \"/data/output/pickle-files/extraction/\"),\n",
    "        \"NLP_PICKLE_OUTPUT_DIR\" : Path(ROOT + \"/data/output/pickle-files/nlp/\"),\n",
    "        \"MLMODEL_PICKLE_OUTPUT_DIR\" : Path(ROOT + \"/data/output/pickle-files/mach-learning/\"),\n",
    "        \"DESC_STATS_OUTPUT_DIR\" : Path(ROOT + \"/data/output/csv-files/\"),\n",
    "        \"RMD_OUTPUT_DIR\" : Path(ROOT + \"/data/output/rmd-files/\"),\n",
    "        \"TIDY_OUTPUT_DIR\" : Path(ROOT + \"/data/output/tidy-files/\")\n",
    "    },\n",
    "    \"model-hyperparameters\" : {\n",
    "        \"DECISION_TREE_PARAMETERS\": {\n",
    "            \"criterion\" : \"gini\",\n",
    "            \"splitter\" : \"best\",\n",
    "            \"max_depth\" : 10,\n",
    "            \"max_features\" : \"sqrt\",\n",
    "            \"min_samples_split\" : 3,\n",
    "            \"min_samples_leaf\" : 4,\n",
    "            \"class_weight\" : \"balanced\"   \n",
    "        },\n",
    "        \"LOG_REG_PARAMETERS\" : {\n",
    "            \"penalty\" : \"l2\",\n",
    "            \"C\" : 11.288378916846883,\n",
    "            \"solver\" : \"liblinear\",\n",
    "            \"max_iter\": 500\n",
    "        },\n",
    "        \"SVM_PARAMETERS\" : {\n",
    "            \"kernel\" : \"linear\",\n",
    "            \"C\" : 0.0018329807108324356,\n",
    "            \"degree\" : 0,\n",
    "            \"gamma\" : \"scale\",\n",
    "            \"cache_size\" : 100,\n",
    "            \"class_weight\" : \"balanced\"\n",
    "        },\n",
    "        \"KNN_PARAMETERS\" : {\n",
    "            \"n_neighbors\": 20,\n",
    "            \"weights\" : \"distance\",\n",
    "            \"algorithm\" : \"auto\",\n",
    "            \"leaf_size\" : 10,\n",
    "            \"p\" : 2,\n",
    "            \"n_jobs\" : -1\n",
    "        },\n",
    "         \"XGB_PARAMETERS\" : {\n",
    "            \"learning_rate\" : 0.1,\n",
    "            \"max_depth\" : 2,\n",
    "            \"min_child_weight\" : 2,\n",
    "            \"n_estimators\" : 200,\n",
    "            \"nthread\" : 1,\n",
    "            \"objective\" : \"binary:logistic\", #\"multi:softmax\", # use \"binary:logistic\" for binary tasks, \"multi:softmax\" for multiclass\n",
    "            \"gamma\" : 0.2,\n",
    "            \"subsample\" : 0.3,\n",
    "            \"colsample_bytree\" : 0.2,\n",
    "            \"eval_metric\" : \"mlogloss\",\n",
    "            \"use_label_encoder\" : False\n",
    "        }   \n",
    "    },\n",
    "    \"parameter-tuning\" : {\n",
    "        \"DECISION_TREE_TUNING\" : {\n",
    "            \"criterion\" : [\"gini\", \"entropy\"],\n",
    "            \"splitter\" : [\"best\", \"random\"],\n",
    "            \"max_depth\" : [5, 10, 15, 20, 25, 30, 35, 40, 50, 60, 70],\n",
    "            \"min_samples_split\" : [2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16],\n",
    "            \"min_samples_leaf\" : [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "            \"max_features\" : [\"auto\", \"sqrt\"],\n",
    "            \"class_weight\" : [\"balanced\"]\n",
    "        },\n",
    "        \"LOG_REG_TUNING\" : [\n",
    "            {\"penalty\": [\"l1\", \"l2\"],\n",
    "             \"C\" : np.logspace(-4, 4, 20),\n",
    "             \"solver\": [\"liblinear\", \"saga\"],\n",
    "             \"max_iter\" : [500, 1000, 2000, 5000]},\n",
    "            {\"penalty\" : [\"l2\"],\n",
    "            \"C\" : np.logspace(-4, 4, 20),\n",
    "            \"solver\" : [\"lbfgs\", \"newton-cg\", \"sag\"],\n",
    "            \"max_iter\" : [500, 1000, 2000, 5000]}\n",
    "        ],\n",
    "        \"SVM_TUNING\" : {\n",
    "            \"kernel\" : [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "            \"C\" : np.logspace(-4, 4, 20),\n",
    "            \"degree\" : [0, 1, 2, 3, 4, 5],\n",
    "            \"gamma\" : [\"scale\", \"auto\"],\n",
    "            \"cache_size\" : [100, 200, 300, 400, 500],\n",
    "            \"class_weight\": [\"balanced\"]    \n",
    "        },\n",
    "        \"KNN_TUNING\" : {\n",
    "            \"n_neighbors\" : [5, 10, 15, 20, 25, 30, 35, 40],\n",
    "            \"weights\" : [\"uniform\", \"distance\"],\n",
    "            \"algorithm\" : [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "            \"leaf_size\" : [10, 20, 30, 40, 50, 60],\n",
    "            \"p\" : [1, 2],\n",
    "            \"n_jobs\" : [-1]\n",
    "        },\n",
    "        \"XGB_TUNING\" : {\n",
    "            \"max_depth\" : [2, 3, 4, 5],\n",
    "            \"min_child_weight\" : [2, 3, 4, 5],\n",
    "            \"n_estimators\" : [50, 100, 150, 200],\n",
    "            \"nthread\" : [1, 2, 3],\n",
    "            \"objective\" : [\"binary:logistic\"], # binary:logistic for binary tasks, multi:softmax for multiclass\n",
    "            \"gamma\" : [0.1, 0.2, 0.3],\n",
    "            \"subsample\" : [0.1, 0.2, 0.3],\n",
    "            \"colsample_bytree\" : [0.1, 0.2, 0.3],\n",
    "            \"eval_metric\" : [\"mlogloss\"],\n",
    "            \"use_label_encoder\" : [False] \n",
    "        }\n",
    "    },\n",
    "    \"RUN_DATETIME\" : datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    \"NGRAM\" : (1, 3),\n",
    "    \"DETECTABLE_CLASSES\": [\"BREAST\", \"BOTTLE\", \"EXPRESS/PUMP\", \"NA\"],\n",
    "    \"CONCAT_CLASS\" : {\"FEEDING\": [\"BREAST\", \"BOTTLE\", \"EXPRESS/PUMP\"]},\n",
    "    \"MATRIX_TYPE\" : \"tf-idf\", # Can be \"tf-idf\" or \"count\"\n",
    "    \"REFINEMENT\" : \"none\", # Can be \"shared\", \"unique\", or \"none\" (default)\n",
    "    \"MIN_DOC_FREQ\": 30,\n",
    "    \"TEST_SIZE\" : 0.20,\n",
    "    \"VALIDATION_SIZE\" : 0.25,\n",
    "    \"CROSS_VALIDATIONS\": 5,\n",
    "    \"RANDOM_STATE\" : 22\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dd6cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_hyperparameters = {\n",
    "    \n",
    "    \"trial-1-uni\": {\n",
    "        \"DECISION_TREE_PARAMETERS\": {\n",
    "            \"criterion\" : \"entropy\",\n",
    "            \"splitter\" : \"best\",\n",
    "            \"max_depth\" : 50,\n",
    "            \"max_features\" : \"auto\",\n",
    "            \"min_samples_split\" : 14,\n",
    "            \"min_samples_leaf\" : 8,\n",
    "            \"class_weight\" : \"balanced\"   \n",
    "        },\n",
    "        \"LOG_REG_PARAMETERS\" : {\n",
    "            \"penalty\" : \"l2\",\n",
    "            \"C\" : 11.288378916846883,\n",
    "            \"solver\" : \"liblinear\",\n",
    "            \"max_iter\": 500\n",
    "        },\n",
    "        \"SVM_PARAMETERS\" : {\n",
    "            \"kernel\" : \"poly\",\n",
    "            \"C\" : 4.281332398719396,\n",
    "            \"degree\" : 1,\n",
    "            \"gamma\" : \"scale\",\n",
    "            \"cache_size\" : 100,\n",
    "            \"class_weight\" : \"balanced\"\n",
    "        },\n",
    "        \"KNN_PARAMETERS\" : {\n",
    "            \"n_neighbors\": 10,\n",
    "            \"weights\" : \"distance\",\n",
    "            \"algorithm\" : \"ball_tree\",\n",
    "            \"leaf_size\" : 10,\n",
    "            \"p\" : 2,\n",
    "            \"n_jobs\" : -1\n",
    "        },\n",
    "         \"XGB_PARAMETERS\" : {\n",
    "            \"learning_rate\" : 0.1,\n",
    "            \"max_depth\" : 3,\n",
    "            \"min_child_weight\" : 2,\n",
    "            \"n_estimators\" : 100,\n",
    "            \"nthread\" : 1,\n",
    "            \"objective\" : \"binary:logistic\", #\"multi:softmax\", # use \"binary:logistic\" for binary tasks, \"multi:softmax\" for multiclass\n",
    "            \"gamma\" : 0.2,\n",
    "            \"subsample\" : 0.3,\n",
    "            \"colsample_bytree\" : 0.3,\n",
    "            \"eval_metric\" : \"mlogloss\",\n",
    "            \"use_label_encoder\" : False\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"trial-1-hybrid\": {\n",
    "        \"DECISION_TREE_PARAMETERS\": {\n",
    "            \"criterion\" : \"entropy\",\n",
    "            \"splitter\" : \"random\",\n",
    "            \"max_depth\" : 50,\n",
    "            \"max_features\" : \"auto\",\n",
    "            \"min_samples_split\" : 12,\n",
    "            \"min_samples_leaf\" : 3,\n",
    "            \"class_weight\" : \"balanced\"   \n",
    "        },\n",
    "        \"LOG_REG_PARAMETERS\" : {\n",
    "            \"penalty\" : \"l2\",\n",
    "            \"C\" : 11.288378916846883,\n",
    "            \"solver\" : \"liblinear\",\n",
    "            \"max_iter\": 500\n",
    "        },\n",
    "        \"SVM_PARAMETERS\" : {\n",
    "            \"kernel\" : \"sigmoid\",\n",
    "            \"C\" : 1.623776739188721,\n",
    "            \"degree\" : 0,\n",
    "            \"gamma\" : \"scale\",\n",
    "            \"cache_size\" : 100,\n",
    "            \"class_weight\" : \"balanced\"\n",
    "        },\n",
    "        \"KNN_PARAMETERS\" : {\n",
    "            \"n_neighbors\": 10,\n",
    "            \"weights\" : \"distance\",\n",
    "            \"algorithm\" : \"auto\",\n",
    "            \"leaf_size\" : 10,\n",
    "            \"p\" : 2,\n",
    "            \"n_jobs\" : -1\n",
    "        },\n",
    "         \"XGB_PARAMETERS\" : {\n",
    "            \"learning_rate\" : 0.1,\n",
    "            \"max_depth\" : 4,\n",
    "            \"min_child_weight\" : 2,\n",
    "            \"n_estimators\" : 100,\n",
    "            \"nthread\" : 1,\n",
    "            \"objective\" : \"binary:logistic\", #\"multi:softmax\", # use \"binary:logistic\" for binary tasks, \"multi:softmax\" for multiclass\n",
    "            \"gamma\" : 0.3,\n",
    "            \"subsample\" : 0.3,\n",
    "            \"colsample_bytree\" : 0.2,\n",
    "            \"eval_metric\" : \"mlogloss\",\n",
    "            \"use_label_encoder\" : False\n",
    "        } \n",
    "    },\n",
    "    \n",
    "    \"trial-2-uni\": {\n",
    "        \"DECISION_TREE_PARAMETERS\": {\n",
    "            \"criterion\" : \"entropy\",\n",
    "            \"splitter\" : \"best\",\n",
    "            \"max_depth\" : 5,\n",
    "            \"max_features\" : \"auto\",\n",
    "            \"min_samples_split\" : 7,\n",
    "            \"min_samples_leaf\" : 2,\n",
    "            \"class_weight\" : \"balanced\"   \n",
    "        },\n",
    "        \"LOG_REG_PARAMETERS\" : {\n",
    "            \"penalty\" : \"l1\",\n",
    "            \"C\" : 4.281332398719396,\n",
    "            \"solver\" : \"liblinear\",\n",
    "            \"max_iter\": 500\n",
    "        },\n",
    "        \"SVM_PARAMETERS\" : {\n",
    "            \"kernel\" : \"poly\",\n",
    "            \"C\" : 4.281332398719396,\n",
    "            \"degree\" : 1,\n",
    "            \"gamma\" : \"scale\",\n",
    "            \"cache_size\" : 100,\n",
    "            \"class_weight\" : \"balanced\"\n",
    "        },\n",
    "        \"KNN_PARAMETERS\" : {\n",
    "            \"n_neighbors\": 20,\n",
    "            \"weights\" : \"distance\",\n",
    "            \"algorithm\" : \"auto\",\n",
    "            \"leaf_size\" : 10,\n",
    "            \"p\" : 2,\n",
    "            \"n_jobs\" : -1\n",
    "        },\n",
    "         \"XGB_PARAMETERS\" : {\n",
    "            \"learning_rate\" : 0.1,\n",
    "            \"max_depth\" : 3,\n",
    "            \"min_child_weight\" : 2,\n",
    "            \"n_estimators\" : 150,\n",
    "            \"nthread\" : 1,\n",
    "            \"objective\" : \"multi:softmax\", #\"multi:softmax\", # use \"binary:logistic\" for binary tasks, \"multi:softmax\" for multiclass\n",
    "            \"gamma\" : 0.1,\n",
    "            \"subsample\" : 0.3,\n",
    "            \"colsample_bytree\" : 0.2,\n",
    "            \"eval_metric\" : \"mlogloss\",\n",
    "            \"use_label_encoder\" : False\n",
    "        } \n",
    "    },\n",
    "    \n",
    "    \"trial-2-hybrid\": {\n",
    "        \"DECISION_TREE_PARAMETERS\": {\n",
    "            \"criterion\" : \"entropy\",\n",
    "            \"splitter\" : \"best\",\n",
    "            \"max_depth\" : 5,\n",
    "            \"max_features\" : \"auto\",\n",
    "            \"min_samples_split\" : 2,\n",
    "            \"min_samples_leaf\" : 8,\n",
    "            \"class_weight\" : \"balanced\"   \n",
    "        },\n",
    "        \"LOG_REG_PARAMETERS\" : {\n",
    "            \"penalty\" : \"l2\",\n",
    "            \"C\" : 4.281332398719396,\n",
    "            \"solver\" : \"liblinear\",\n",
    "            \"max_iter\": 500\n",
    "        },\n",
    "        \"SVM_PARAMETERS\" : {\n",
    "            \"kernel\" : \"sigmoid\",\n",
    "            \"C\" : 1.623776739188721,\n",
    "            \"degree\" : 0,\n",
    "            \"gamma\" : \"scale\",\n",
    "            \"cache_size\" : 100,\n",
    "            \"class_weight\" : \"balanced\"\n",
    "        },\n",
    "        \"KNN_PARAMETERS\" : {\n",
    "            \"n_neighbors\": 15,\n",
    "            \"weights\" : \"distance\",\n",
    "            \"algorithm\" : \"ball_tree\",\n",
    "            \"leaf_size\" : 10,\n",
    "            \"p\" : 2,\n",
    "            \"n_jobs\" : -1\n",
    "        },\n",
    "         \"XGB_PARAMETERS\" : {\n",
    "            \"learning_rate\" : 0.1,\n",
    "            \"max_depth\" : 2,\n",
    "            \"min_child_weight\" : 2,\n",
    "            \"n_estimators\" : 200,\n",
    "            \"nthread\" : 1,\n",
    "            \"objective\" : \"multi:softmax\", #\"multi:softmax\", # use \"binary:logistic\" for binary tasks, \"multi:softmax\" for multiclass\n",
    "            \"gamma\" : 0.2,\n",
    "            \"subsample\" : 0.3,\n",
    "            \"colsample_bytree\" : 0.2,\n",
    "            \"eval_metric\" : \"mlogloss\",\n",
    "            \"use_label_encoder\" : False\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"trial-3-uni\": {\n",
    "        \"DECISION_TREE_PARAMETERS\": {\n",
    "            \"criterion\" : \"gini\",\n",
    "            \"splitter\" : \"best\",\n",
    "            \"max_depth\" : 10,\n",
    "            \"max_features\" : \"auto\",\n",
    "            \"min_samples_split\" : 9,\n",
    "            \"min_samples_leaf\" : 4,\n",
    "            \"class_weight\" : \"balanced\"   \n",
    "        },\n",
    "        \"LOG_REG_PARAMETERS\" : {\n",
    "            \"penalty\" : \"l2\",\n",
    "            \"C\" : 4.281332398719396,\n",
    "            \"solver\" : \"liblinear\",\n",
    "            \"max_iter\": 500\n",
    "        },\n",
    "        \"SVM_PARAMETERS\" : {\n",
    "            \"kernel\" : \"rbf\",\n",
    "            \"C\" : 4.281332398719396,\n",
    "            \"degree\" : 0,\n",
    "            \"gamma\" : \"scale\",\n",
    "            \"cache_size\" : 100,\n",
    "            \"class_weight\" : \"balanced\"\n",
    "        },\n",
    "        \"KNN_PARAMETERS\" : {\n",
    "            \"n_neighbors\": 15,\n",
    "            \"weights\" : \"distance\",\n",
    "            \"algorithm\" : \"ball_tree\",\n",
    "            \"leaf_size\" : 10,\n",
    "            \"p\" : 2,\n",
    "            \"n_jobs\" : -1\n",
    "        },\n",
    "         \"XGB_PARAMETERS\" : {\n",
    "            \"learning_rate\" : 0.1,\n",
    "            \"max_depth\" : 5,\n",
    "            \"min_child_weight\" : 2,\n",
    "            \"n_estimators\" : 150,\n",
    "            \"nthread\" : 1,\n",
    "            \"objective\" : \"multi:softmax\", #\"multi:softmax\", # use \"binary:logistic\" for binary tasks, \"multi:softmax\" for multiclass\n",
    "            \"gamma\" : 0.2,\n",
    "            \"subsample\" : 0.3,\n",
    "            \"colsample_bytree\" : 0.3,\n",
    "            \"eval_metric\" : \"mlogloss\",\n",
    "            \"use_label_encoder\" : False\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"trial-3-hybrid\": {\n",
    "        \"DECISION_TREE_PARAMETERS\": {\n",
    "            \"criterion\" : \"gini\",\n",
    "            \"splitter\" : \"best\",\n",
    "            \"max_depth\" : 20,\n",
    "            \"max_features\" : \"sqrt\",\n",
    "            \"min_samples_split\" : 9,\n",
    "            \"min_samples_leaf\" : 2,\n",
    "            \"class_weight\" : \"balanced\"   \n",
    "        },\n",
    "        \"LOG_REG_PARAMETERS\" : {\n",
    "            \"penalty\" : \"l2\",\n",
    "            \"C\" : 11.288378916846883,\n",
    "            \"solver\" : \"liblinear\",\n",
    "            \"max_iter\": 500\n",
    "        },\n",
    "        \"SVM_PARAMETERS\" : {\n",
    "            \"kernel\" : \"rbf\",\n",
    "            \"C\" : 1438.44988828766,\n",
    "            \"degree\" : 0,\n",
    "            \"gamma\" : \"auto\",\n",
    "            \"cache_size\" : 100,\n",
    "            \"class_weight\" : \"balanced\"\n",
    "        },\n",
    "        \"KNN_PARAMETERS\" : {\n",
    "            \"n_neighbors\": 15,\n",
    "            \"weights\" : \"distance\",\n",
    "            \"algorithm\" : \"auto\",\n",
    "            \"leaf_size\" : 10,\n",
    "            \"p\" : 2,\n",
    "            \"n_jobs\" : -1\n",
    "        },\n",
    "         \"XGB_PARAMETERS\" : {\n",
    "            \"learning_rate\" : 0.1,\n",
    "            \"max_depth\" : 5,\n",
    "            \"min_child_weight\" : 2,\n",
    "            \"n_estimators\" : 150,\n",
    "            \"nthread\" : 1,\n",
    "            \"objective\" : \"multi:softmax\", #\"multi:softmax\", # use \"binary:logistic\" for binary tasks, \"multi:softmax\" for multiclass\n",
    "            \"gamma\" : 0.2,\n",
    "            \"subsample\" : 0.3,\n",
    "            \"colsample_bytree\" : 0.3,\n",
    "            \"eval_metric\" : \"mlogloss\",\n",
    "            \"use_label_encoder\" : False\n",
    "        }\n",
    "    }      \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098a3b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "# -------------------------- GENERAL USE FUNCTIONS --------------------------- #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "def timer(method):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.perf_counter()\n",
    "        x = method(*args, **kwargs)\n",
    "        end_time = time.perf_counter()\n",
    "        time_elapsed = end_time - start_time\n",
    "        print(\"Run time (seconds): \", time_elapsed)\n",
    "        return x\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00d4bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "# -------------------------- EXTRACTION FUNCTIONS ---------------------------- #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "def xml_extraction(data_directory:str, concat_class: dict):\n",
    "    \"\"\"\n",
    "    Loops through all annotated xml documents to collect values for baby\n",
    "    and note numbers, annotated classification, and text content. Creates two\n",
    "    dictionaries, both using the same keys.\n",
    "\n",
    "    Parameters:\n",
    "    data_directory (str): A string denoting the path where xml files are\n",
    "    located.\n",
    "\n",
    "    concat_class (dict): Used to concatenate the data of similar classifiers\n",
    "    into a single classifier.\n",
    "\n",
    "    Returns:\n",
    "    class_dict (dict): A dictionary with keys that are tuples containing\n",
    "    patient ID and note number, and values that are the manually annotated\n",
    "    classification of the respective clinical note in string format.\n",
    "\n",
    "    text_dict (dict): A dictionary with keys that are tuples containing\n",
    "    patient ID and note number, and values that are the contents of the\n",
    "    clinical notes themselves in string format.\n",
    "    \"\"\"\n",
    "\n",
    "    class_dict = {}\n",
    "    text_dict = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(data_directory):\n",
    "        for file in sorted(files):\n",
    "            if file.endswith('.xml'):\n",
    "                etree = et.parse(os.path.join(root, file))\n",
    "                baby_note = etree.findall('document//passage')[-1].find(\"text\").text\n",
    "                baby_note = tuple(re.findall('[\\d]+', baby_note))\n",
    "                text_content = etree.findall('document//passage')[0].find(\"text\").text\n",
    "\n",
    "                if baby_note not in class_dict.keys():\n",
    "                    class_dict[baby_note] = set()\n",
    "                if baby_note not in text_dict.keys():\n",
    "                    text_dict[baby_note] = text_content\n",
    "\n",
    "                annotations = etree.findall('document//annotation')\n",
    "\n",
    "                for annotation in annotations:\n",
    "                    annotation_type = annotation.find(\"infon[@key='type']\").text\n",
    "                    annotation_text = annotation.find(\"text\").text\n",
    "                    if annotation_type == 'FEED_CLASS':\n",
    "                        class_dict[baby_note].add(annotation_text)\n",
    "\n",
    "    class_dict = {key:element for key, value in class_dict.items() if len(value) == 1 for element in value}\n",
    "    \n",
    "    # Probably should return here and create new function with lower code\n",
    "    # can be put in \"if concat_class:\" block\n",
    "    \n",
    "    for k, v in concat_class.items():\n",
    "        for elem in v:\n",
    "            class_dict = {key:( k if value == elem else value) for key, value in class_dict.items()}\n",
    "    \n",
    "    class_dict = {key: value for key, value in class_dict.items() if value in config[\"DETECTABLE_CLASSES\"] or value in config[\"CONCAT_CLASS\"]}\n",
    "\n",
    "    return class_dict, text_dict\n",
    "\n",
    "\n",
    "def structure_dataframe(class_dict:dict, text_dict:dict):\n",
    "    \"\"\"\n",
    "    Takes two dictionaries that share keys, but with different values, and\n",
    "    creates a pandas dataframe. Keys are set as the dataframe's index.\n",
    "\n",
    "    Parameters:\n",
    "    class_dict (dict): dictionary that containes key:value with a tuple of\n",
    "    patient ID and note number as the key and the manually annotated\n",
    "    classification, as a string, for the respective note as the value.\n",
    "\n",
    "    text_dict (dict): dictionary that containes key:value with a tuple of\n",
    "    patient ID and note number as the key and the contents of the note in string\n",
    "    format as the value.\n",
    "\n",
    "    Returns:\n",
    "    dataframe (pandas.DataFrame): A dataframe that combines the two dictionaries.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = [class_dict, text_dict]\n",
    "    data = {key:[d[key] for d in data] for key in data[0]}\n",
    "    dataframe = pd.DataFrame.from_dict(data,\n",
    "                                orient='index',\n",
    "                                columns=['--classification--', 'content'])\n",
    "    dataframe.index.name='id_note'\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de0830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "# ----------------------------- NLP FUNCTIONS -------------------------------- #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "def text_preprocessing(corpus):\n",
    "    \"\"\"\n",
    "    Alters a list of document-length strings, modifying each string to remove\n",
    "    redacted tokens, irregular whitespace, punctuation, English stop words, and\n",
    "    tokens that are composed solely of numbers. All tokens are adjusted to be\n",
    "    lowercased.\n",
    "\n",
    "    Parameters:\n",
    "    corpus (list): A list of document-length strings\n",
    "\n",
    "    Returns:\n",
    "    corpus (list): A list of nested lists containing the strings of individual\n",
    "    tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    for i, note in enumerate(corpus):\n",
    "        note = [word for word in note.split(' ') if not re.match(\"(\\[[\\*]+[\\w]+[\\*]+\\])\", word)]\n",
    "        note = ' '.join(note)\n",
    "        note = re.sub(r'[^\\w\\s]', '', note)\n",
    "        note = [word for word in note.split(' ') if not re.match(\"([\\*]+)\", word)]\n",
    "        note = [word for word in note if word != '']\n",
    "        note = [word.lower() for word in note]\n",
    "        note = [word for word in note if word not in stop_words]\n",
    "        corpus[i] = [word for word in note if not re.match(\"([0-9]+)\", word)]\n",
    "\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def split_dataframe(dataframe, detectable_classes: list):\n",
    "    \"\"\"\n",
    "    Divides a single pandas dataframe into a list of dataframes, segregated by\n",
    "    manually annotated response variables in string format.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pandas.DataFrame): A dataframe containing a column of\n",
    "    string-represented classifiers for the data.\n",
    "\n",
    "    concat_class (bool): Used to concatenate the data of two similar classifiers\n",
    "    into a single classifier.\n",
    "\n",
    "    Returns:\n",
    "    dataframe_list (list): A list containing dataframes separated by\n",
    "    classification\n",
    "    \"\"\"\n",
    "    \n",
    "    dataframe_list = []\n",
    "    for elem in dataframe[\"--classification--\"].unique():\n",
    "        df = pd.DataFrame([row for row in dataframe.itertuples() if row[1] == elem], \n",
    "                          columns=[\"Index\", \"--classification--\", \"content\"])\n",
    "        dataframe_list.append(df) \n",
    "        \n",
    "    return dataframe_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba47734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "# ----------------------- FEATURE CREATION FUNCTIONS ------------------------- #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "def term_matrix(dataframe, corpus, ngram_range, matrix_type):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for i, note in enumerate(corpus):\n",
    "        corpus[i] = ' '.join(note)\n",
    "\n",
    "    dataframe.drop(dataframe.columns[1], axis=1, inplace=True)\n",
    "    dataframe['content'] = corpus\n",
    "    \n",
    "    if matrix_type == \"tf-idf\":\n",
    "        vectorizer = TfidfVectorizer(analyzer='word', ngram_range=ngram_range, min_df=config[\"MIN_DOC_FREQ\"])\n",
    "    elif matrix_type == \"count\":\n",
    "        vectorizer = CountVectorizer(analyzer='word', ngram_range=ngram_range, min_df=config[\"MIN_DOC_FREQ\"])\n",
    "    else:\n",
    "        pass\n",
    "        # need to put a proper RaiseError here\n",
    "        # Something like 'config[\"MATRIX_TYPE\"] is not a valid argument' then end run\n",
    "\n",
    "    X = vectorizer.fit_transform(dataframe['content'])\n",
    "    \n",
    "    bow_dataframe = pd.DataFrame(X.toarray(), \n",
    "                                 columns=vectorizer.get_feature_names_out(),\n",
    "                                 index=dataframe.index)\n",
    "    \n",
    "    bow_dataframe['--classification--'] = dataframe['--classification--']\n",
    "    \n",
    "    return bow_dataframe\n",
    "                                     \n",
    "\n",
    "def shared_term_matrix(dataframe, corpus, ngram_range, matrix_type):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "                                     \n",
    "    for i, note in enumerate(corpus):\n",
    "        corpus[i] = ' '.join(note)\n",
    "\n",
    "    dataframe.drop(dataframe.columns[1], axis=1, inplace=True)\n",
    "    dataframe['content'] = corpus\n",
    "    \n",
    "    separated_dataframe = split_dataframe(dataframe, \n",
    "                                          detectable_classes=config[\"DETECTABLE_CLASSES\"])\n",
    "\n",
    "    if matrix_type == \"tf-idf\":\n",
    "        vectorizer = TfidfVectorizer(analyzer='word', ngram_range=ngram_range, min_df=config[\"MIN_DOC_FREQ\"])\n",
    "    elif matrix_type == \"count\":\n",
    "        vectorizer = CountVectorizer(analyzer='word', ngram_range=ngram_range, min_df=config[\"MIN_DOC_FREQ\"])\n",
    "    else:\n",
    "        pass\n",
    "        # need to put a proper RaiseError here\n",
    "        # Something like 'config[\"MATRIX_TYPE\"] is not a valid argument' then end run\n",
    "    \n",
    "    bow_dataframe_list = []\n",
    "    for df in separated_dataframe:\n",
    "        X = vectorizer.fit_transform(df['content'])\n",
    "        bow_dataframe = pd.DataFrame(X.toarray(), \n",
    "                                     columns=vectorizer.get_feature_names_out(),\n",
    "                                     index=df.index)\n",
    "        \n",
    "        bow_dataframe['--classification--'] = df['--classification--']\n",
    "        bow_dataframe_list.append(bow_dataframe)\n",
    "    \n",
    "    features = bow_dataframe_list[0].columns\n",
    "    \n",
    "    for i in range(len(bow_dataframe_list)):\n",
    "        features = np.intersect1d(features, bow_dataframe_list[i].columns)\n",
    "    features = features.tolist()\n",
    "    \n",
    "    output_dataframe = bow_dataframe_list[0][features]\n",
    "    \n",
    "    for i in range(len(bow_dataframe_list)-1):\n",
    "        output_dataframe = pd.merge(output_dataframe[features], bow_dataframe_list[i+1][features], how='outer')\n",
    "        \n",
    "    return output_dataframe\n",
    "                \n",
    "def unique_term_matrix(dataframe, corpus, ngram_range, concat_class):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    ngram_features = ngram_creation(corpus, ngram_range=ngram_range)\n",
    "\n",
    "    dataframe.drop(dataframe.columns[1], axis=1, inplace=True)\n",
    "\n",
    "    for i in range(len(ngram_features)):\n",
    "        dataframe[str(i+1)+'-gram'] = ngram_features[i]\n",
    "\n",
    "\n",
    "    separated_dataframe = split_dataframe(dataframe, concat_class=concat_class)\n",
    "\n",
    "    vocab_list = token_refinement(separated_dataframe, ngram_range=ngram_range)\n",
    "    vocab_list = unique_elements(vocab_list)\n",
    "    vocab_list = merge_vocab(vocab_list, ngram_range=ngram_range)\n",
    "\n",
    "    bow_dataframe = bag_of_words(dataframe, vocab_list, tf_idf=config[\"TF_IDF\"])\n",
    "    \n",
    "    return bow_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ad2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "# ----------------------- MACHINE LEARNING FUNCTIONS ------------------------- #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# Need to set this tuning function up to have an if statement for each possible model\n",
    "# For XGB, need to map the classes to the note_dataframe[\"--classifications--\"].values()\n",
    "# or some variation of that.\n",
    "\n",
    "\n",
    "\n",
    "def hyperparameter_tuning(X, y, model, param_grid):\n",
    "#     f1 = make_scorer(f1_score , average='weighted')\n",
    "    int_encoder = {value: i for i, value in enumerate(y.unique().tolist())}\n",
    "    \n",
    "    y = y.replace(y.unique().tolist(), list(range(len(y.unique()))))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=config[\"TEST_SIZE\"], random_state=config[\"RANDOM_STATE\"])\n",
    "    \n",
    "    gscv = GridSearchCV(model, \n",
    "                        param_grid=param_grid,\n",
    "                        scoring=\"accuracy\",\n",
    "                        cv=config[\"CROSS_VALIDATIONS\"],\n",
    "                        verbose=True,\n",
    "                        n_jobs=-1,\n",
    "                        error_score=\"raise\")\n",
    "    \n",
    "    gscv.fit(X_train, y_train)\n",
    "\n",
    "    print(gscv.best_params_)\n",
    "    print(gscv.best_score_)\n",
    "    print(gscv.best_estimator_)\n",
    "    \n",
    "\n",
    "def feature_selection(X, y, model):\n",
    "    \n",
    "    int_encoder = {value: i for i, value in enumerate(y.unique().tolist())}\n",
    "    \n",
    "    y = y.replace(y.unique().tolist(), list(range(len(y.unique()))))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=config[\"TEST_SIZE\"], random_state=config[\"RANDOM_STATE\"])\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    columns = []\n",
    "    feature_importance = []\n",
    "    \n",
    "    for i, column in enumerate(X_train):\n",
    "        columns.append(column)\n",
    "        feature_importance.append(model.feature_importances_[i])\n",
    "    \n",
    "    feature_dataframe = zip(columns, feature_importance)\n",
    "    feature_dataframe = pd.DataFrame(feature_dataframe, columns=[\"features\", \"feature_importance\"])\n",
    "    \n",
    "    feature_dataframe = feature_dataframe.sort_values(\"feature_importance\", ascending=False)\n",
    "\n",
    "    for row in feature_dataframe.itertuples():\n",
    "        \n",
    "#         print(type(row[1])) #string\n",
    "#         print(type(row[2])) #float\n",
    "        if row[2] > 0:\n",
    "            print(\"Feature: {} \\t Importance: {}\".format(row[1], row[2]))\n",
    "\n",
    "\n",
    "def model_decision_tree(X, y):\n",
    "    \n",
    "    model = DecisionTreeClassifier(criterion=model_hyperparameters[\"trial-1-uni\"][\"DECISION_TREE_PARAMETERS\"][\"criterion\"],\n",
    "                                   splitter=model_hyperparameters[\"trial-1-uni\"][\"DECISION_TREE_PARAMETERS\"][\"splitter\"],\n",
    "                                   max_depth=model_hyperparameters[\"trial-1-uni\"][\"DECISION_TREE_PARAMETERS\"][\"max_depth\"],\n",
    "                                   max_features=model_hyperparameters[\"trial-1-uni\"][\"DECISION_TREE_PARAMETERS\"][\"max_features\"],\n",
    "                                   min_samples_split=model_hyperparameters[\"trial-1-uni\"][\"DECISION_TREE_PARAMETERS\"][\"min_samples_split\"],\n",
    "                                   min_samples_leaf=model_hyperparameters[\"trial-1-uni\"][\"DECISION_TREE_PARAMETERS\"][\"min_samples_leaf\"],\n",
    "                                   class_weight=model_hyperparameters[\"trial-1-uni\"][\"DECISION_TREE_PARAMETERS\"][\"class_weight\"],\n",
    "                                   random_state=config[\"RANDOM_STATE\"])\n",
    "    \n",
    "#     model = DecisionTreeClassifier(criterion=model_hyperparameters[\"trial-1-hybrid\"][\"DECISION_TREE_PARAMETERS\"][\"criterion\"],\n",
    "#                                    splitter=model_hyperparameters[\"trial-1-hybrid\"][\"DECISION_TREE_PARAMETERS\"][\"splitter\"],\n",
    "#                                    max_depth=model_hyperparameters[\"trial-1-hybrid\"][\"DECISION_TREE_PARAMETERS\"][\"max_depth\"],\n",
    "#                                    max_features=model_hyperparameters[\"trial-1-hybrid\"][\"DECISION_TREE_PARAMETERS\"][\"max_features\"],\n",
    "#                                    min_samples_split=model_hyperparameters[\"trial-1-hybrid\"][\"DECISION_TREE_PARAMETERS\"][\"min_samples_split\"],\n",
    "#                                    min_samples_leaf=model_hyperparameters[\"trial-1-hybrid\"][\"DECISION_TREE_PARAMETERS\"][\"min_samples_leaf\"],\n",
    "#                                    class_weight=model_hyperparameters[\"trial-1-hybrid\"][\"DECISION_TREE_PARAMETERS\"][\"class_weight\"],\n",
    "#                                    random_state=config[\"RANDOM_STATE\"])\n",
    "    \n",
    "#     model = DecisionTreeClassifier(criterion=config[\"model_hyperparameters\"][\"DECISION_TREE_PARAMETERS\"][\"criterion\"],\n",
    "#                                    splitter=config[\"model_hyperparameters\"][\"DECISION_TREE_PARAMETERS\"][\"splitter\"],\n",
    "#                                    max_depth=config[\"model_hyperparameters\"][\"DECISION_TREE_PARAMETERS\"][\"max_depth\"],\n",
    "#                                    max_features=config[\"model_hyperparameters\"][\"DECISION_TREE_PARAMETERS\"][\"max_features\"],\n",
    "#                                    min_samples_split=config[\"model_hyperparameters\"][\"DECISION_TREE_PARAMETERS\"][\"min_samples_split\"],\n",
    "#                                    min_samples_leaf=config[\"model_hyperparameters\"][\"DECISION_TREE_PARAMETERS\"][\"min_samples_leaf\"],\n",
    "#                                    class_weight=config[\"model_hyperparameters\"][\"DECISION_TREE_PARAMETERS\"][\"class_weight\"],\n",
    "#                                    random_state=config[\"RANDOM_STATE\"])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=config[\"TEST_SIZE\"], random_state=config[\"RANDOM_STATE\"])\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    report = classification_report(y_test.tolist(), model.predict(X_test).tolist(), output_dict=True)\n",
    "    \n",
    "    report_dataframe = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    model_save_file = os.path.join(config[\"directories\"][\"MLMODEL_PICKLE_OUTPUT_DIR\"], \n",
    "                                   config[\"RUN_DATETIME\"] + \"-\" + str(type(model).__name__) + \".pkl\")\n",
    "    \n",
    "    csv_save_file = os.path.join(config[\"directories\"][\"DESC_STATS_OUTPUT_DIR\"],\n",
    "                                 config['RUN_DATETIME'] + \"-\" + str(type(model).__name__) + \"-stats.csv\")\n",
    "    \n",
    "    pickle.dump(model, open(model_save_file, \"wb\"))\n",
    "    report_dataframe.to_csv(csv_save_file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def model_logistic_regression(X, y):\n",
    "    \n",
    "    model = LogisticRegression(penalty=model_hyperparameters[\"trial-1-uni\"][\"LOG_REG_PARAMETERS\"][\"penalty\"],\n",
    "                               C=model_hyperparameters[\"trial-1-uni\"][\"LOG_REG_PARAMETERS\"][\"C\"],\n",
    "                               solver=model_hyperparameters[\"trial-1-uni\"][\"LOG_REG_PARAMETERS\"][\"solver\"],\n",
    "                               max_iter=model_hyperparameters[\"trial-1-uni\"][\"LOG_REG_PARAMETERS\"][\"max_iter\"],\n",
    "                               random_state=config[\"RANDOM_STATE\"])\n",
    "    \n",
    "#     model = LogisticRegression(penalty=model_hyperparameters[\"trial-1-hybrid\"][\"LOG_REG_PARAMETERS\"][\"penalty\"],\n",
    "#                                C=model_hyperparameters[\"trial-1-hybrid\"][\"LOG_REG_PARAMETERS\"][\"C\"],\n",
    "#                                solver=model_hyperparameters[\"trial-1-hybrid\"][\"LOG_REG_PARAMETERS\"][\"solver\"],\n",
    "#                                max_iter=model_hyperparameters[\"trial-1-hybrid\"][\"LOG_REG_PARAMETERS\"][\"max_iter\"],\n",
    "#                                random_state=config[\"RANDOM_STATE\"])\n",
    "    \n",
    "#     model = LogisticRegression(penalty=config[\"model_hyperparameters\"][\"LOG_REG_PARAMETERS\"][\"penalty\"],\n",
    "#                                C=config[\"model_hyperparameters\"][\"LOG_REG_PARAMETERS\"][\"C\"],\n",
    "#                                solver=config[\"model_hyperparameters\"][\"LOG_REG_PARAMETERS\"][\"solver\"],\n",
    "#                                max_iter=config[\"model_hyperparameters\"][\"LOG_REG_PARAMETERS\"][\"max_iter\"],\n",
    "#                                random_state=config[\"RANDOM_STATE\"])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=config[\"TEST_SIZE\"], random_state=config[\"RANDOM_STATE\"])\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    report = classification_report(y_test.tolist(), model.predict(X_test).tolist(), output_dict=True)\n",
    "    \n",
    "    report_dataframe = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    model_save_file = os.path.join(config[\"directories\"][\"MLMODEL_PICKLE_OUTPUT_DIR\"], \n",
    "                                   config[\"RUN_DATETIME\"] + \"-\" + str(type(model).__name__) + \".pkl\")\n",
    "    \n",
    "    csv_save_file = os.path.join(config[\"directories\"][\"DESC_STATS_OUTPUT_DIR\"],\n",
    "                                 config['RUN_DATETIME'] + \"-\" + str(type(model).__name__) + \"-stats.csv\")\n",
    "    \n",
    "    pickle.dump(model, open(model_save_file, \"wb\"))\n",
    "    report_dataframe.to_csv(csv_save_file)\n",
    "    \n",
    "    \n",
    "def model_svm (X, y):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    model = SVC(kernel=model_hyperparameters[\"trial-1-uni\"][\"SVM_PARAMETERS\"][\"kernel\"],\n",
    "                C=model_hyperparameters[\"trial-1-uni\"][\"SVM_PARAMETERS\"][\"C\"],\n",
    "                degree=model_hyperparameters[\"trial-1-uni\"][\"SVM_PARAMETERS\"][\"degree\"],\n",
    "                gamma=model_hyperparameters[\"trial-1-uni\"][\"SVM_PARAMETERS\"][\"gamma\"],\n",
    "                cache_size=model_hyperparameters[\"trial-1-uni\"][\"SVM_PARAMETERS\"][\"cache_size\"],\n",
    "                class_weight=model_hyperparameters[\"trial-1-uni\"][\"SVM_PARAMETERS\"][\"class_weight\"],\n",
    "                random_state=config[\"RANDOM_STATE\"])\n",
    "    \n",
    "#     model = SVC(kernel=model_hyperparameters[\"trial-1-hybrid\"][\"SVM_PARAMETERS\"][\"kernel\"],\n",
    "#                 C=model_hyperparameters[\"trial-1-hybrid\"][\"SVM_PARAMETERS\"][\"C\"],\n",
    "#                 degree=model_hyperparameters[\"trial-1-hybrid\"][\"SVM_PARAMETERS\"][\"degree\"],\n",
    "#                 gamma=model_hyperparameters[\"trial-1-hybrid\"][\"SVM_PARAMETERS\"][\"gamma\"],\n",
    "#                 cache_size=model_hyperparameters[\"trial-1-hybrid\"][\"SVM_PARAMETERS\"][\"cache_size\"],\n",
    "#                 class_weight=model_hyperparameters[\"trial-1-hybrid\"][\"SVM_PARAMETERS\"][\"class_weight\"],\n",
    "#                 random_state=config[\"RANDOM_STATE\"])\n",
    "    \n",
    "#     model = SVC(kernel=config[\"model_hyperparameters\"][\"SVM_PARAMETERS\"][\"kernel\"],\n",
    "#                 C=config[\"model_hyperparameters\"][\"SVM_PARAMETERS\"][\"C\"],\n",
    "#                 degree=config[\"model_hyperparameters\"][\"SVM_PARAMETERS\"][\"degree\"],\n",
    "#                 gamma=config[\"model_hyperparameters\"][\"SVM_PARAMETERS\"][\"gamma\"],\n",
    "#                 cache_size=config[\"model_hyperparameters\"][\"SVM_PARAMETERS\"][\"cache_size\"],\n",
    "#                 class_weight=config[\"model_hyperparameters\"][\"SVM_PARAMETERS\"][\"class_weight\"],\n",
    "#                 random_state=config[\"RANDOM_STATE\"])\n",
    "    \n",
    "    X = preprocessing.scale(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=config[\"TEST_SIZE\"], random_state=config[\"RANDOM_STATE\"])\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    report = classification_report(y_test.tolist(), model.predict(X_test).tolist(), output_dict=True)\n",
    "    \n",
    "    report_dataframe = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    model_save_file = os.path.join(config[\"directories\"][\"MLMODEL_PICKLE_OUTPUT_DIR\"], \n",
    "                                   config[\"RUN_DATETIME\"] + \"-\" + str(type(model).__name__) + \".pkl\")\n",
    "    \n",
    "    csv_save_file = os.path.join(config[\"directories\"][\"DESC_STATS_OUTPUT_DIR\"],\n",
    "                                 config['RUN_DATETIME'] + \"-\" + str(type(model).__name__) + \"-stats.csv\")\n",
    "    \n",
    "    pickle.dump(model, open(model_save_file, \"wb\"))\n",
    "    report_dataframe.to_csv(csv_save_file)\n",
    "\n",
    "def model_knn(X, y):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    model = KNeighborsClassifier(n_neighbors=model_hyperparameters[\"trial-1-uni\"][\"KNN_PARAMETERS\"][\"n_neighbors\"],\n",
    "                                 weights=model_hyperparameters[\"trial-1-uni\"][\"KNN_PARAMETERS\"][\"weights\"],\n",
    "                                 algorithm=model_hyperparameters[\"trial-1-uni\"][\"KNN_PARAMETERS\"][\"algorithm\"],\n",
    "                                 leaf_size=model_hyperparameters[\"trial-1-uni\"][\"KNN_PARAMETERS\"][\"leaf_size\"],\n",
    "                                 p=model_hyperparameters[\"trial-1-uni\"][\"KNN_PARAMETERS\"][\"p\"],\n",
    "                                 n_jobs=model_hyperparameters[\"trial-1-uni\"][\"KNN_PARAMETERS\"][\"n_jobs\"])\n",
    "    \n",
    "#     model = KNeighborsClassifier(n_neighbors=model_hyperparameters[\"trial-1-hybrid\"][\"KNN_PARAMETERS\"][\"n_neighbors\"],\n",
    "#                                  weights=model_hyperparameters[\"trial-1-hybrid\"][\"KNN_PARAMETERS\"][\"weights\"],\n",
    "#                                  algorithm=model_hyperparameters[\"trial-1-hybrid\"][\"KNN_PARAMETERS\"][\"algorithm\"],\n",
    "#                                  leaf_size=model_hyperparameters[\"trial-1-hybrid\"][\"KNN_PARAMETERS\"][\"leaf_size\"],\n",
    "#                                  p=model_hyperparameters[\"trial-1-hybrid\"][\"KNN_PARAMETERS\"][\"p\"],\n",
    "#                                  n_jobs=model_hyperparameters[\"trial-1-hybrid\"][\"KNN_PARAMETERS\"][\"n_jobs\"])\n",
    "    \n",
    "#     model = KNeighborsClassifier(n_neighbors=config[\"model-hyperparameters\"][\"KNN_PARAMETERS\"][\"n_neighbors\"],\n",
    "#                                  weights=config[\"model-hyperparameters\"][\"KNN_PARAMETERS\"][\"weights\"],\n",
    "#                                  algorithm=config[\"model-hyperparameters\"][\"KNN_PARAMETERS\"][\"algorithm\"],\n",
    "#                                  leaf_size=config[\"model-hyperparameters\"][\"KNN_PARAMETERS\"][\"leaf_size\"],\n",
    "#                                  p=config[\"model-hyperparameters\"][\"KNN_PARAMETERS\"][\"p\"],\n",
    "#                                  n_jobs=config[\"model-hyperparameters\"][\"KNN_PARAMETERS\"][\"n_jobs\"])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=config[\"TEST_SIZE\"], random_state=config[\"RANDOM_STATE\"])\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    report = classification_report(y_test.tolist(), model.predict(X_test).tolist(), output_dict=True)\n",
    "    \n",
    "    report_dataframe = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    model_save_file = os.path.join(config[\"directories\"][\"MLMODEL_PICKLE_OUTPUT_DIR\"], \n",
    "                                   config[\"RUN_DATETIME\"] + \"-\" + str(type(model).__name__) + \".pkl\")\n",
    "    \n",
    "    csv_save_file = os.path.join(config[\"directories\"][\"DESC_STATS_OUTPUT_DIR\"],\n",
    "                                 config['RUN_DATETIME'] + \"-\" + str(type(model).__name__) + \"-stats.csv\")\n",
    "    \n",
    "    pickle.dump(model, open(model_save_file, \"wb\"))\n",
    "    report_dataframe.to_csv(csv_save_file)\n",
    "    \n",
    "def model_xgboost(X, y):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    model = XGBClassifier(learning_rate=model_hyperparameters[\"trial-1-uni\"][\"XGB_PARAMETERS\"][\"learning_rate\"],\n",
    "                          max_depth=model_hyperparameters[\"trial-1-uni\"][\"XGB_PARAMETERS\"][\"max_depth\"],\n",
    "                          min_child_weight=model_hyperparameters[\"trial-1-uni\"][\"XGB_PARAMETERS\"][\"min_child_weight\"],\n",
    "                          n_estimators=model_hyperparameters[\"trial-1-uni\"][\"XGB_PARAMETERS\"][\"n_estimators\"],\n",
    "                          nthread=model_hyperparameters[\"trial-1-uni\"][\"XGB_PARAMETERS\"][\"nthread\"],\n",
    "                          objective=model_hyperparameters[\"trial-1-uni\"][\"XGB_PARAMETERS\"][\"objective\"],\n",
    "                          gamma=model_hyperparameters[\"trial-1-uni\"][\"XGB_PARAMETERS\"][\"gamma\"],\n",
    "                          subsample=model_hyperparameters[\"trial-1-uni\"][\"XGB_PARAMETERS\"][\"subsample\"],\n",
    "                          colsample_bytree=model_hyperparameters[\"trial-1-uni\"][\"XGB_PARAMETERS\"][\"colsample_bytree\"],\n",
    "                          eval_metric=model_hyperparameters[\"trial-1-uni\"][\"XGB_PARAMETERS\"][\"eval_metric\"],\n",
    "                          use_label_encoder=model_hyperparameters[\"trial-1-uni\"][\"XGB_PARAMETERS\"][\"use_label_encoder\"],\n",
    "                          random_state=model_hyperparameters[\"RANDOM_STATE\"])\n",
    "    \n",
    "#     model = XGBClassifier(learning_rate=model_hyperparameters[\"trial-1-hybrid\"][\"XGB_PARAMETERS\"][\"learning_rate\"],\n",
    "#                           max_depth=model_hyperparameters[\"trial-1-hybrid\"][\"XGB_PARAMETERS\"][\"max_depth\"],\n",
    "#                           min_child_weight=model_hyperparameters[\"trial-1-hybrid\"][\"XGB_PARAMETERS\"][\"min_child_weight\"],\n",
    "#                           n_estimators=model_hyperparameters[\"trial-1-hybrid\"][\"XGB_PARAMETERS\"][\"n_estimators\"],\n",
    "#                           nthread=model_hyperparameters[\"trial-1-hybrid\"][\"XGB_PARAMETERS\"][\"nthread\"],\n",
    "#                           objective=model_hyperparameters[\"trial-1-hybrid\"][\"XGB_PARAMETERS\"][\"objective\"],\n",
    "#                           gamma=model_hyperparameters[\"trial-1-hybrid\"][\"XGB_PARAMETERS\"][\"gamma\"],\n",
    "#                           subsample=model_hyperparameters[\"trial-1-hybrid\"][\"XGB_PARAMETERS\"][\"subsample\"],\n",
    "#                           colsample_bytree=model_hyperparameters[\"trial-1-hybrid\"][\"XGB_PARAMETERS\"][\"colsample_bytree\"],\n",
    "#                           eval_metric=model_hyperparameters[\"trial-1-hybrid\"][\"XGB_PARAMETERS\"][\"eval_metric\"],\n",
    "#                           use_label_encoder=model_hyperparameters[\"trial-1-hybrid\"][\"XGB_PARAMETERS\"][\"use_label_encoder\"],\n",
    "#                           random_state=model_hyperparameters[\"RANDOM_STATE\"])   \n",
    "    \n",
    "#     model = XGBClassifier(learning_rate=config[\"model-hyperparameters\"][\"XGB_PARAMETERS\"][\"learning_rate\"],\n",
    "#                           max_depth=config[\"model-hyperparameters\"][\"XGB_PARAMETERS\"][\"max_depth\"],\n",
    "#                           min_child_weight=config[\"model-hyperparameters\"][\"XGB_PARAMETERS\"][\"min_child_weight\"],\n",
    "#                           n_estimators=config[\"model-hyperparameters\"][\"XGB_PARAMETERS\"][\"n_estimators\"],\n",
    "#                           nthread=config[\"model-hyperparameters\"][\"XGB_PARAMETERS\"][\"nthread\"],\n",
    "#                           objective=config[\"model-hyperparameters\"][\"XGB_PARAMETERS\"][\"objective\"],\n",
    "#                           gamma=config[\"model-hyperparameters\"][\"XGB_PARAMETERS\"][\"gamma\"],\n",
    "#                           subsample=config[\"model-hyperparameters\"][\"XGB_PARAMETERS\"][\"subsample\"],\n",
    "#                           colsample_bytree=config[\"model-hyperparameters\"][\"XGB_PARAMETERS\"][\"colsample_bytree\"],\n",
    "#                           eval_metric=config[\"model-hyperparameters\"][\"XGB_PARAMETERS\"][\"eval_metric\"],\n",
    "#                           use_label_encoder=config[\"model-hyperparameters\"][\"XGB_PARAMETERS\"][\"use_label_encoder\"],\n",
    "#                           random_state=config[\"RANDOM_STATE\"])\n",
    "    \n",
    "    int_decoder = {str(i): value for i, value in enumerate(y.unique().tolist())}\n",
    "    \n",
    "    y = y.replace(y.unique().tolist(), list(range(len(y.unique()))))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=config[\"TEST_SIZE\"], random_state=config[\"RANDOM_STATE\"])\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    report = classification_report(y_test.tolist(), model.predict(X_test).tolist(), output_dict=True)\n",
    "    report = {(int_decoder[key] if key in int_decoder else key):value for key, value in report.items()}\n",
    "    \n",
    "    report_dataframe = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    model_save_file = os.path.join(config[\"directories\"][\"MLMODEL_PICKLE_OUTPUT_DIR\"], \n",
    "                                   config[\"RUN_DATETIME\"] + \"-\" + str(type(model).__name__) + \".pkl\")\n",
    "    \n",
    "    csv_save_file = os.path.join(config[\"directories\"][\"DESC_STATS_OUTPUT_DIR\"],\n",
    "                                 config['RUN_DATETIME'] + \"-\" + str(type(model).__name__) + \"-stats.csv\")\n",
    "    \n",
    "    pickle.dump(model, open(model_save_file, \"wb\"))\n",
    "    report_dataframe.to_csv(csv_save_file)\n",
    "    \n",
    "                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af14e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "# ------------------------- EXTRACTION EXECUTABLES --------------------------- #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "@timer\n",
    "def extr_execute():\n",
    "    \"\"\"Executes the extraction process of the TaCLE package.\"\"\"\n",
    "#     with open(\"./../../config.json\", 'r') as jsonfile:\n",
    "#         config = json.load(jsonfile)\n",
    "\n",
    "    class_dict, text_dict = xml_extraction(config[\"directories\"][\"V0_DATA_IMPORT_DIR\"], concat_class=config['CONCAT_CLASS'])\n",
    "\n",
    "    note_dataframe = structure_dataframe(class_dict, text_dict)\n",
    "    \n",
    "    save_file = Path(str(config['directories']['EXTR_PICKLE_OUTPUT_DIR']) + ('/' + config['RUN_DATETIME'] + '-extr.pkl'))\n",
    "    note_dataframe.to_pickle(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9515709",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    extr_execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b38ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "# ----------------------------- NLP EXECUTABLES ------------------------------ #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "@timer\n",
    "def nlp_execute(refinement: str=\"none\"):\n",
    "    \"\"\"Executes the natural language processing tasks of the TaCLE package.\"\"\"\n",
    "\n",
    "#     with open(\"./../config.json\", 'r') as jsonfile:\n",
    "#         config = json.load(jsonfile)\n",
    "\n",
    "#     nltk.download('stopwords')\n",
    "\n",
    "    load_file = Path(str(config['directories']['EXTR_PICKLE_OUTPUT_DIR']) + ('/' + config['RUN_DATETIME'] + '-extr.pkl'))\n",
    "\n",
    "    note_dataframe = pd.read_pickle(load_file)\n",
    "\n",
    "    note_corpus = note_dataframe['content'].tolist()\n",
    "    note_corpus = text_preprocessing(note_corpus)\n",
    "\n",
    "    print(note_dataframe['--classification--'].value_counts())\n",
    "   \n",
    "    \n",
    "    if refinement == \"none\":\n",
    "        bow_dataframe = term_matrix(note_dataframe, \n",
    "                                    note_corpus, \n",
    "                                    ngram_range=config[\"NGRAM\"],\n",
    "                                    matrix_type=config[\"MATRIX_TYPE\"])\n",
    "\n",
    "    elif refinement == \"shared\":\n",
    "        bow_dataframe = shared_term_matrix(note_dataframe,\n",
    "                                           note_corpus,\n",
    "                                           ngram_range=config[\"NGRAM\"],\n",
    "                                           matrix_type=config[\"MATRIX_TYPE\"])\n",
    "    \n",
    "    elif refinement == \"unique\":\n",
    "        bow_dataframe = unique_term_matrix(note_dataframe, \n",
    "                                           note_corpus, \n",
    "                                           ngram_range=config[\"NGRAM\"],\n",
    "                                           matrix_type=config[\"MATRIX_TYPE\"])\n",
    "        \n",
    "    save_file = Path(str(config[\"directories\"][\"NLP_PICKLE_OUTPUT_DIR\"]) + ('/' + config[\"RUN_DATETIME\"] + '-nlp.pkl'))\n",
    "    bow_dataframe.to_pickle(save_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62636f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    nlp_execute(refinement=config[\"REFINEMENT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28f8e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "# ------------------------- MODEL TUNING EXECUTABLES ------------------------- #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "@timer\n",
    "def ml_tuning(model, matrix_type: str):\n",
    "    \n",
    "    load_file = Path(str(config['directories']['NLP_PICKLE_OUTPUT_DIR']) + ('/' + config['RUN_DATETIME'] + '-nlp.pkl'))\n",
    "\n",
    "    bow_dataframe = pd.read_pickle(load_file)\n",
    "    \n",
    "    X = bow_dataframe.iloc[:, 0:bow_dataframe.shape[1]-1]\n",
    "    y = bow_dataframe.iloc[:, bow_dataframe.shape[1]-1]\n",
    "    \n",
    "    hyperparameter_tuning(X, y, model, matrix_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f400dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "# ----------------------- MACHINE LEARNING EXECUTABLES ----------------------- #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "@timer\n",
    "def ml_execute(matrix_type: str):\n",
    "    \"\"\"Executes the machine learning tasks of the TaCLE package.\"\"\"\n",
    "    \n",
    "#             with open(\"./../config.json\", 'r') as jsonfile:\n",
    "#                 config = json.load(jsonfile)\n",
    "\n",
    "    load_file = Path(str(config['directories']['NLP_PICKLE_OUTPUT_DIR']) + ('/' + config['RUN_DATETIME'] + '-nlp.pkl'))\n",
    "\n",
    "    bow_dataframe = pd.read_pickle(load_file)\n",
    "    \n",
    "    X = bow_dataframe.iloc[:, 0:bow_dataframe.shape[1]-1]\n",
    "    y = bow_dataframe.iloc[:, bow_dataframe.shape[1]-1]\n",
    "    \n",
    "    model_decision_tree(X, y)\n",
    "    model_knn(X, y)\n",
    "    model_logistic_regression(X, y)\n",
    "    model_svm(X, y)\n",
    "    model_xgboost(X, y)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     y = pd.get_dummies(y)\n",
    "    \n",
    "#     for i in range(1, 6):\n",
    "#         model_decision_tree(X, y)\n",
    "#         print(' ---------- ')\n",
    "    \n",
    "#     feature_selection(X, y, model)\n",
    "\n",
    "#     X_train, X_val, X_test, y_train, y_val, y_test = split_data(bow_dataframe, \n",
    "#                                                                 test_size=config[\"TEST_SIZE\"], \n",
    "#                                                                 val_size=config[\"VALIDATION_SIZE\"],\n",
    "#                                                                 random_state=config[\"RANDOM_STATE\"])\n",
    "\n",
    "    \n",
    "    \n",
    "#     y_train_pred = model.predict_proba(X_train)[:, 1]\n",
    "#     y_val_pred = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "#     print(\"AUC Train: {:.4f}\\nAUC Valid: {:.4f}\".format(roc_auc_score(y_train, y_train_pred),\n",
    "#                                                         roc_auc_score(y_test, y_val_pred)))\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba5bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "        \n",
    "    load_file = Path(str(config['directories']['NLP_PICKLE_OUTPUT_DIR']) + ('/' + config['RUN_DATETIME'] + '-nlp.pkl'))\n",
    "\n",
    "    bow_dataframe = pd.read_pickle(load_file)\n",
    "    \n",
    "    X = bow_dataframe.iloc[:, 0:bow_dataframe.shape[1]-1]\n",
    "    y = bow_dataframe.iloc[:, bow_dataframe.shape[1]-1]\n",
    "    \n",
    "    print(\"DecisionTree\")\n",
    "    hyperparameter_tuning(X, y, model=DecisionTreeClassifier(), param_grid=config[\"parameter-tuning\"][\"DECISION_TREE_TUNING\"])\n",
    "    print(\"----------\\n\")\n",
    "        \n",
    "    print(\"KNeighbors\")\n",
    "    hyperparameter_tuning(X, y, model=KNeighborsClassifier(), param_grid=config[\"parameter-tuning\"][\"KNN_TUNING\"])\n",
    "    print(\"----------\\n\")\n",
    "        \n",
    "    print(\"LogisticRegression\")\n",
    "    hyperparameter_tuning(X, y, model=LogisticRegression(), param_grid=config[\"parameter-tuning\"][\"LOG_REG_TUNING\"])\n",
    "    print(\"----------\\n\")\n",
    "        \n",
    "    print(\"SVC\")\n",
    "    hyperparameter_tuning(X, y, model=SVC(), param_grid=config[\"parameter-tuning\"][\"SVM_TUNING\"])\n",
    "    print(\"----------\\n\")\n",
    "    \n",
    "    print(\"XGBoost\")\n",
    "    hyperparameter_tuning(X, y, model=XGBClassifier(use_label_encoder=False), param_grid=config[\"parameter-tuning\"][\"XGB_TUNING\"])\n",
    "    print(\"----------\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1339ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ml_execute(matrix_type=config[\"MATRIX_TYPE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6369bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary (feeding, na), unigram\n",
    "\n",
    "# DecisionTree\n",
    "# Fitting 5 folds for each of 9504 candidates, totalling 47520 fits\n",
    "# {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 50, 'max_features': 'auto', 'min_samples_leaf': 8, 'min_samples_split': 14, 'splitter': 'best'}\n",
    "# 0.8501298701298701\n",
    "\n",
    "# KNeighbors\n",
    "# Fitting 5 folds for each of 768 candidates, totalling 3840 fits\n",
    "# {'algorithm': 'ball_tree', 'leaf_size': 10, 'n_jobs': -1, 'n_neighbors': 10, 'p': 2, 'weights': 'distance'}\n",
    "# 0.8397653958944282\n",
    "\n",
    "# LogisticRegression\n",
    "# Fitting 5 folds for each of 560 candidates, totalling 2800 fits\n",
    "# {'C': 11.288378916846883, 'max_iter': 500, 'penalty': 'l2', 'solver': 'liblinear'}\n",
    "# 0.8733724340175953\n",
    "\n",
    "# SVC\n",
    "# Fitting 5 folds for each of 4800 candidates, totalling 24000 fits\n",
    "# {'C': 4.281332398719396, 'cache_size': 100, 'class_weight': 'balanced', 'degree': 1, 'gamma': 'scale', 'kernel': 'poly'}\n",
    "# 0.8720737327188939\n",
    "\n",
    "# XGBoost\n",
    "# Fitting 5 folds for each of 5184 candidates, totalling 25920 fits\n",
    "# {'colsample_bytree': 0.3, 'eval_metric': 'mlogloss', 'gamma': 0.2, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 100, 'nthread': 1, 'objective': 'binary:logistic', 'subsample': 0.3, 'use_label_encoder': False}\n",
    "# 0.8669208211143695\n",
    "\n",
    "# ------------------------------\n",
    "# Binary (feeding, na), hybrid-gram\n",
    "\n",
    "# DecisionTree\n",
    "# Fitting 5 folds for each of 9504 candidates, totalling 47520 fits\n",
    "# {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 50, 'max_features': 'auto', 'min_samples_leaf': 3, 'min_samples_split': 12, 'splitter': 'random'}\n",
    "# 0.8462421449518225\n",
    "\n",
    "# KNeighbors\n",
    "# Fitting 5 folds for each of 768 candidates, totalling 3840 fits\n",
    "# {'algorithm': 'auto', 'leaf_size': 10, 'n_jobs': -1, 'n_neighbors': 10, 'p': 2, 'weights': 'distance'}\n",
    "# 0.8462337662337662\n",
    "\n",
    "# LogisticRegression\n",
    "# Fitting 5 folds for each of 560 candidates, totalling 2800 fits\n",
    "# {'C': 11.288378916846883, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n",
    "# 0.8682111436950146\n",
    "\n",
    "# SVC\n",
    "# Fitting 5 folds for each of 4800 candidates, totalling 24000 fits\n",
    "# {'C': 1.623776739188721, 'cache_size': 100, 'class_weight': 'balanced', 'degree': 0, 'gamma': 'scale', 'kernel': 'sigmoid'}\n",
    "# 0.8746627565982404\n",
    "\n",
    "# XGBoost\n",
    "# Fitting 5 folds for each of 5184 candidates, totalling 25920 fits\n",
    "# {'colsample_bytree': 0.2, 'eval_metric': 'mlogloss', 'gamma': 0.3, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 100, 'nthread': 1, 'objective': 'binary:logistic', 'subsample': 0.3, 'use_label_encoder': False}\n",
    "# 0.8656388772517805\n",
    "\n",
    "# ------------------------------\n",
    "# Multiclass (breast, bottle, na), unigram\n",
    "\n",
    "# DecisionTree\n",
    "# Fitting 5 folds for each of 9504 candidates, totalling 47520 fits\n",
    "# {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 5, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 7, 'splitter': 'best'}\n",
    "# 0.8023460410557185\n",
    "\n",
    "# KNearestNeighbors\n",
    "# Fitting 5 folds for each of 768 candidates, totalling 3840 fits\n",
    "# {'algorithm': 'auto', 'leaf_size': 10, 'n_jobs': -1, 'n_neighbors': 20, 'p': 2, 'weights': 'distance'}\n",
    "# 0.7764893171344784\n",
    "\n",
    "# LogisticRegression\n",
    "# Fitting 5 folds for each of 560 candidates, totalling 2800 fits\n",
    "# {'C': 4.281332398719396, 'max_iter': 500, 'penalty': 'l1', 'solver': 'liblinear'}\n",
    "# 0.8500879765395893\n",
    "\n",
    "# SVC\n",
    "# Fitting 5 folds for each of 4800 candidates, totalling 24000 fits\n",
    "# {'C': 4.281332398719396, 'cache_size': 100, 'class_weight': 'balanced', 'degree': 1, 'gamma': 'scale', 'kernel': 'poly'}\n",
    "# 0.8333305404273146\n",
    "\n",
    "# XGBoost\n",
    "# Fitting 5 folds for each of 5184 candidates, totalling 25920 fits\n",
    "# {'colsample_bytree': 0.2, 'eval_metric': 'mlogloss', 'gamma': 0.1, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 150, 'nthread': 1, 'objective': 'multi:softmax', 'subsample': 0.3, 'use_label_encoder': False}\n",
    "# 0.8669208211143694\n",
    "\n",
    "# ------------------------------\n",
    "#Multiclass (breast, bottle, na), hybrid-gram\n",
    "\n",
    "# DecisionTree\n",
    "# Fitting 5 folds for each of 9504 candidates, totalling 47520 fits\n",
    "# {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 5, 'max_features': 'auto', 'min_samples_leaf': 8, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "# 0.7997235023041475\n",
    "\n",
    "# KNeighbors\n",
    "# Fitting 5 folds for each of 768 candidates, totalling 3840 fits\n",
    "# {'algorithm': 'ball_tree', 'leaf_size': 10, 'n_jobs': -1, 'n_neighbors': 15, 'p': 2, 'weights': 'distance'}\n",
    "# 0.7958609132802681\n",
    "\n",
    "# LogisticRegression\n",
    "# Fitting 5 folds for each of 560 candidates, totalling 2800 fits\n",
    "# {'C': 4.281332398719396, 'max_iter': 500, 'penalty': 'l2', 'solver': 'liblinear'}\n",
    "# 0.854017595307918\n",
    "\n",
    "# SVC\n",
    "# Fitting 5 folds for each of 4800 candidates, totalling 24000 fits\n",
    "# {'C': 1.623776739188721, 'cache_size': 100, 'class_weight': 'balanced', 'degree': 0, 'gamma': 'scale', 'kernel': 'sigmoid'}\n",
    "# 0.8552911604524507\n",
    "\n",
    "# XGBoost\n",
    "# Fitting 5 folds for each of 5184 candidates, totalling 25920 fits\n",
    "# {'colsample_bytree': 0.2, 'eval_metric': 'mlogloss', 'gamma': 0.2, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 200, 'nthread': 1, 'objective': 'multi:softmax', 'subsample': 0.3, 'use_label_encoder': False}\n",
    "# 0.8720988688730623\n",
    "\n",
    "# ------------------------------\n",
    "#Multiclass (breast, bottle, express, na), unigram\n",
    "\n",
    "# DecisionTree\n",
    "# Fitting 5 folds for each of 9504 candidates, totalling 47520 fits\n",
    "# {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 4, 'min_samples_split': 9, 'splitter': 'best'}\n",
    "# 0.745429409300377\n",
    "\n",
    "# KNearestNeighbors\n",
    "# Fitting 5 folds for each of 768 candidates, totalling 3840 fits\n",
    "# {'algorithm': 'auto', 'leaf_size': 10, 'n_jobs': -1, 'n_neighbors': 15, 'p': 2, 'weights': 'distance'}\n",
    "# 0.760980310012568\n",
    "\n",
    "#LogisticRegression\n",
    "# Fitting 5 folds for each of 560 candidates, totalling 2800 fits\n",
    "# {'C': 1.623776739188721, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
    "# 0.8346041055718475\n",
    "\n",
    "#SVC\n",
    "# Fitting 5 folds for each of 4800 candidates, totalling 24000 fits\n",
    "# {'C': 4.281332398719396, 'cache_size': 100, 'class_weight': 'balanced', 'degree': 0, 'gamma': 'scale', 'kernel': 'rbf'}\n",
    "# 0.8087892752408882\n",
    "\n",
    "#XGBoost\n",
    "# Fitting 5 folds for each of 5184 candidates, totalling 25920 fits\n",
    "# {'colsample_bytree': 0.2, 'eval_metric': 'mlogloss', 'gamma': 0.2, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 150, 'nthread': 1, 'objective': 'multi:softmax', 'subsample': 0.3, 'use_label_encoder': False}\n",
    "# 0.854025974025974\n",
    "\n",
    "# ------------------------------\n",
    "#Multiclass (breast, bottle, express, na), hybrid-gram\n",
    "\n",
    "#DecisionTree\n",
    "# Fitting 5 folds for each of 9504 candidates, totalling 47520 fits\n",
    "# {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 9, 'splitter': 'best'}\n",
    "# 0.7428906577293674\n",
    "\n",
    "#KNearestNeighbors\n",
    "# Fitting 5 folds for each of 768 candidates, totalling 3840 fits\n",
    "# {'algorithm': 'auto', 'leaf_size': 10, 'n_jobs': -1, 'n_neighbors': 15, 'p': 2, 'weights': 'distance'}\n",
    "# 0.7842312526183495\n",
    "\n",
    "#LogisticRegression\n",
    "# Fitting 5 folds for each of 560 candidates, totalling 2800 fits\n",
    "# {'C': 11.288378916846883, 'max_iter': 500, 'penalty': 'l2', 'solver': 'liblinear'}\n",
    "# 0.8385085881860075\n",
    "\n",
    "#SVC\n",
    "# Fitting 5 folds for each of 4800 candidates, totalling 24000 fits\n",
    "# {'C': 1438.44988828766, 'cache_size': 100, 'class_weight': 'balanced', 'degree': 0, 'gamma': 'auto', 'kernel': 'rbf'}\n",
    "# 0.8243150397989109\n",
    "\n",
    "#XGBoost\n",
    "# Fitting 5 folds for each of 5184 candidates, totalling 25920 fits\n",
    "# {'colsample_bytree': 0.3, 'eval_metric': 'mlogloss', 'gamma': 0.2, 'max_depth': 5, 'min_child_weight': 2, 'n_estimators': 150, 'nthread': 1, 'objective': 'multi:softmax', 'subsample': 0.3, 'use_label_encoder': False}\n",
    "# 0.8514453288646837\n",
    "\n",
    "# ------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a9e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     model = LogisticRegression(penalty=config[\"machine-learning\"][\"LOG_REG_PARAMETERS\"][\"penalty\"],\n",
    "#                        C=config[\"machine-learning\"][\"LOG_REG_PARAMETERS\"][\"C\"],\n",
    "#                        solver=config[\"machine-learning\"][\"LOG_REG_PARAMETERS\"][\"solver\"],\n",
    "#                        max_iter=config[\"machine-learning\"][\"LOG_REG_PARAMETERS\"][\"max_iter\"])\n",
    "    \n",
    "#     model = KNeighborsClassifier(n_neightbors=config[\"machine-learning\"][\"KNN_PARAMETERS\"][\"n_neighbors\"],\n",
    "#                                  weights=config[\"machine-learning\"][\"KNN_PARAMETERS\"][\"weights\"],\n",
    "#                                  algorithm=config[\"machine-learning\"][\"KNN_PARAMETERS\"][\"algorithm\"],\n",
    "#                                  leaf_size=config[\"machine-learning\"][\"KNN_PARAMETERS\"][\"leaf_size\"],\n",
    "#                                  p=config[\"machine-learning\"][\"KNN_PARAMETERS\"][\"p\"],\n",
    "#                                  n_jobs=config[\"machine-learning\"][\"KNN_PARAMETERS\"][\"n_jobs\"])\n",
    "\n",
    "#     model = DecisionTreeClassifier(criterion=config[\"machine-learning\"][\"DECISION_TREE_PARAMETERS\"][\"criterion\"],\n",
    "#                                    splitter=config[\"machine-learning\"][\"DECISION_TREE_PARAMETERS\"][\"splitter\"],\n",
    "#                                    max_depth=config[\"machine-learning\"][\"DECISION_TREE_PARAMETERS\"][\"max_depth\"],\n",
    "#                                    max_features=config[\"machine-learning\"][\"DECISION_TREE_PARAMETERS\"][\"max_features\"],\n",
    "#                                    min_samples_split=config[\"machine-learning\"][\"DECISION_TREE_PARAMETERS\"][\"min_samples_split\"],\n",
    "#                                    min_samples_leaf=config[\"machine-learning\"][\"DECISION_TREE_PARAMETERS\"][\"min_samples_leaf\"],\n",
    "#                                    class_weight=config[\"machine-learning\"][\"DECISION_TREE_PARAMETERS\"][\"class_weight\"])\n",
    "    \n",
    "#     model = SVC(kernel=config[\"machine-learning\"][\"SVM_PARAMETERS\"][\"kernel\"],\n",
    "#             C=config[\"machine-learning\"][\"SVM_PARAMETERS\"][\"C\"],\n",
    "#             degree=config[\"machine-learning\"][\"SVM_PARAMETERS\"][\"degree\"],\n",
    "#             gamma=config[\"machine-learning\"][\"SVM_PARAMETERS\"][\"gamma\"],\n",
    "#             cache_size=config[\"machine-learning\"][\"SVM_PARAMETERS\"][\"cache_size\"],\n",
    "#             class_weight=config[\"machine-learning\"][\"SVM_PARAMETERS\"][\"class_weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeed89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799a1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (968, 886)\n",
    "# Fitting 10 folds for each of 9504 candidates, totalling 95040 fits\n",
    "# {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 15, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 5, 'splitter': 'best'}\n",
    "# 0.7895386251076795\n",
    "# DecisionTreeClassifier(class_weight='balanced', criterion='entropy',\n",
    "#                        max_depth=15, max_features='auto', min_samples_leaf=2,\n",
    "#                        min_samples_split=5)\n",
    "#  ---------- \n",
    "# Fitting 10 folds for each of 9504 candidates, totalling 95040 fits\n",
    "# {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 15, 'max_features': 'auto', 'min_samples_leaf': 5, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "# 0.792258076511153\n",
    "# DecisionTreeClassifier(class_weight='balanced', criterion='entropy',\n",
    "#                        max_depth=15, max_features='auto', min_samples_leaf=5)\n",
    "#  ---------- \n",
    "# Fitting 10 folds for each of 9504 candidates, totalling 95040 fits\n",
    "# {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 3, 'splitter': 'best'}\n",
    "# 0.7936551796694611\n",
    "# DecisionTreeClassifier(class_weight='balanced', max_depth=10,\n",
    "#                        max_features='sqrt', min_samples_leaf=4,\n",
    "#                        min_samples_split=3)\n",
    "#  ---------- \n",
    "# Fitting 10 folds for each of 9504 candidates, totalling 95040 fits\n",
    "# {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 9, 'splitter': 'best'}\n",
    "# 0.7931437420045309\n",
    "# DecisionTreeClassifier(class_weight='balanced', max_depth=15,\n",
    "#                        max_features='sqrt', min_samples_leaf=2,\n",
    "#                        min_samples_split=9)\n",
    "#  ---------- \n",
    "# Fitting 10 folds for each of 9504 candidates, totalling 95040 fits\n",
    "# {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 40, 'max_features': 'auto', 'min_samples_leaf': 6, 'min_samples_split': 6, 'splitter': 'best'}\n",
    "# 0.792533826878633\n",
    "# DecisionTreeClassifier(class_weight='balanced', criterion='entropy',\n",
    "#                        max_depth=40, max_features='auto', min_samples_leaf=6,\n",
    "#                        min_samples_split=6)\n",
    "#  ---------- \n",
    "# Run time (seconds):  5859.5219454000035"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
